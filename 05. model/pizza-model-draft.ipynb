{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import Module","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\n\n# Deeplearning\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Data load\nimport os\nimport random\nfrom torch.utils.data import TensorDataset, Dataset # 텐서데이터셋\nfrom torch.utils.data import DataLoader # 데이터로더\nfrom torchvision import datasets, transforms","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:54.658257Z","iopub.execute_input":"2022-08-31T13:23:54.659467Z","iopub.status.idle":"2022-08-31T13:23:57.098823Z","shell.execute_reply.started":"2022-08-31T13:23:54.658793Z","shell.execute_reply":"2022-08-31T13:23:57.097621Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.102810Z","iopub.execute_input":"2022-08-31T13:23:57.103387Z","iopub.status.idle":"2022-08-31T13:23:57.184315Z","shell.execute_reply.started":"2022-08-31T13:23:57.103342Z","shell.execute_reply":"2022-08-31T13:23:57.182026Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Check Data","metadata":{}},{"cell_type":"code","source":"# 이미지 확인하기\nimage=cv2.imread('../input/pizza-not-pizza/pizza_not_pizza/not_pizza/1005746.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) \nImage.fromarray(image) ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.189684Z","iopub.execute_input":"2022-08-31T13:23:57.190094Z","iopub.status.idle":"2022-08-31T13:23:57.422492Z","shell.execute_reply.started":"2022-08-31T13:23:57.190048Z","shell.execute_reply":"2022-08-31T13:23:57.421539Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"cv2 모듈을 사용해 이미지 파일을 불러오면 채널이 BGR으로 등록된다. 이를 cvtColor를 사용해 RGB로 바꿔준다.\n\ncv2.imread('path')를 사용하면 path(경로)에 있는 데이터를 행렬(array)형태로 불러온다.","metadata":{}},{"cell_type":"code","source":"# 데이터 개수 확인\nnot_pizza = os.listdir('../input/pizza-not-pizza/pizza_not_pizza/not_pizza')\n\npizza = os.listdir('../input/pizza-not-pizza/pizza_not_pizza/pizza')\n\nprint('not_pizza list 길이 : ',len(not_pizza));print('pizza list 길이 : ',len(pizza))\n\nprint('\\n')\n\nprint(not_pizza[0]) # 이미지 파일 이름이 리스트에 저장된 것을 확인","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.424604Z","iopub.execute_input":"2022-08-31T13:23:57.425025Z","iopub.status.idle":"2022-08-31T13:23:57.568021Z","shell.execute_reply.started":"2022-08-31T13:23:57.424989Z","shell.execute_reply":"2022-08-31T13:23:57.566918Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"os.listdir(path)는 path 경로에 있는 값들을 리스트화하는 함수이다. ","metadata":{}},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"image=cv2.imread('../input/pizza-not-pizza/pizza_not_pizza/not_pizza/1005746.jpg')\nimage.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.569417Z","iopub.execute_input":"2022-08-31T13:23:57.569965Z","iopub.status.idle":"2022-08-31T13:23:57.582862Z","shell.execute_reply.started":"2022-08-31T13:23:57.569926Z","shell.execute_reply":"2022-08-31T13:23:57.581847Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"PyTorch의 ImageTensor는 **(채널 x 높이 x 너비)** 의 구조를 갖고 있다. \n하지만 위의 코드를 보면 알다시피 python에서 3차원 tensor를 표현할 때는 **(높이 x 너비 x 채널)** 의 구조를 갖는다. 이를 해결하기 위해 **torchvision .transforms**를 사용한다. \n해당 모듈을 사용하면 tensor 구조 뿐만 아니라 밝기 정도 또한  **[0 ~ 255] 에서 [0 ~ 1] 으로 스케일링**된다.","metadata":{}},{"cell_type":"code","source":"#주워니\ndata_transform = {\n    'train' :  transforms.Compose([\n        transforms.Resize(300),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ]),\n    'val' : transforms.Compose([\n        transforms.Resize(300),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n    ])\n}","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.584585Z","iopub.execute_input":"2022-08-31T13:23:57.584939Z","iopub.status.idle":"2022-08-31T13:23:57.591553Z","shell.execute_reply.started":"2022-08-31T13:23:57.584904Z","shell.execute_reply":"2022-08-31T13:23:57.590270Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------\nResize 옵션을 사용해 이미지의 높이와 너비를 300 으로 만들고\n\nRandomCrop 옵션을 사용해 이미지의 높이와 너비를 224로 자른다.\n\n--------","metadata":{}},{"cell_type":"code","source":"path = \"/kaggle/input/pizza-not-pizza/pizza_not_pizza/\"\n\nimages_dataset = {x : datasets.ImageFolder(root = path, transform = data_transform[x]) for x in ['train', 'val']}\n\ntrain_dataloader = DataLoader(images_dataset['train'], batch_size= 3, shuffle=True, num_workers=4)\nval_dataloader = DataLoader(images_dataset['val'], batch_size= 3, shuffle=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.593293Z","iopub.execute_input":"2022-08-31T13:23:57.593729Z","iopub.status.idle":"2022-08-31T13:23:57.627184Z","shell.execute_reply.started":"2022-08-31T13:23:57.593694Z","shell.execute_reply":"2022-08-31T13:23:57.626153Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Model ","metadata":{}},{"cell_type":"code","source":"class CNN(torch.nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        \n        # L1 ImgIn shape=(?, 224, 224, 3)\n        #    Conv     -> (?, 224, 224, 32)\n        #    Pool     -> (?, 112, 112, 32)\n        self.layer1 = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        # L2 ImgIn shape=(?, 112, 112, 32)\n        #    Conv      ->(?, 112, 112, 64)\n        #    Pool      ->(?, 56, 56, 64)\n        self.layer2 = torch.nn.Sequential(\n            torch.nn.Conv2d(32,64, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        \n        # L3 ImgIn shape=(?, 56, 56, 64)\n        #    Conv      ->(?, 56, 56, 128)\n        #    Pool      ->(?, 28, 28, 128)\n        self.layer3 = torch.nn.Sequential(\n            torch.nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            torch.nn.ReLU(),\n            torch.nn.MaxPool2d(kernel_size=2, stride=2))\n        \n        self.fc = torch.nn.Linear(28 * 28 * 128, 10, bias=True)\n        \n        torch.nn.init.xavier_uniform_(self.fc.weight)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = out.view(out.size(0), -1)\n        out = self.fc(out)\n        \n        return out","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.628624Z","iopub.execute_input":"2022-08-31T13:23:57.629041Z","iopub.status.idle":"2022-08-31T13:23:57.639170Z","shell.execute_reply.started":"2022-08-31T13:23:57.629007Z","shell.execute_reply":"2022-08-31T13:23:57.638039Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"model = CNN().to(device)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:23:57.640712Z","iopub.execute_input":"2022-08-31T13:23:57.641380Z","iopub.status.idle":"2022-08-31T13:24:00.789267Z","shell.execute_reply.started":"2022-08-31T13:23:57.641344Z","shell.execute_reply":"2022-08-31T13:24:00.788297Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nepochs = 100\nbatch_size = 10","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:24:00.793165Z","iopub.execute_input":"2022-08-31T13:24:00.794085Z","iopub.status.idle":"2022-08-31T13:24:00.799134Z","shell.execute_reply.started":"2022-08-31T13:24:00.794045Z","shell.execute_reply":"2022-08-31T13:24:00.797682Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss().to(device) # 비용 함수에 소프트맥스 함수 포함되어져 있음.\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:24:00.800685Z","iopub.execute_input":"2022-08-31T13:24:00.801460Z","iopub.status.idle":"2022-08-31T13:24:00.809556Z","shell.execute_reply.started":"2022-08-31T13:24:00.801396Z","shell.execute_reply":"2022-08-31T13:24:00.808444Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# ckeck loss","metadata":{}},{"cell_type":"code","source":"\n'''\nloss_arr = []\nfor i in range(epochs):\n    for j, [image,label] in enumerate(train_dataloader):\n        x = image.to(device)\n        y = label.to(device)\n        \n        optimizer.zero_grad()\n        \n        output = model.forward(x)\n        \n        loss = criterion(output, y)\n        loss.backward()\n        \n        optimizer.step()\n        \n        if j % 10 == 0 :\n            print(loss)\n            loss_arr.append(loss.cpu().detach().numpy())        \n'''","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:24:00.810717Z","iopub.execute_input":"2022-08-31T13:24:00.812690Z","iopub.status.idle":"2022-08-31T13:24:00.822835Z","shell.execute_reply.started":"2022-08-31T13:24:00.812651Z","shell.execute_reply":"2022-08-31T13:24:00.821855Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# ckeck accuracy","metadata":{}},{"cell_type":"code","source":"y_true = list()\ny_pred = list()\n\nwith torch.no_grad():\n    for j, [image,label] in enumerate(val_dataloader):\n        x = image.to(device)\n        y = label.to(device)\n        \n        pred = model(x).argmax(dim=-1)\n        \n        for i in range(len(pred)):\n            y_true.append(y[i].item())\n            y_pred.append(pred[i].item())\n            \nnp.mean(y_pred)\n\nnp.mean(y_true)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:24:00.824199Z","iopub.execute_input":"2022-08-31T13:24:00.826653Z","iopub.status.idle":"2022-08-31T13:24:06.440572Z","shell.execute_reply.started":"2022-08-31T13:24:00.826625Z","shell.execute_reply":"2022-08-31T13:24:06.438646Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"loss_arr = []\nfor i in range(epochs):\n    for j, [image,label] in enumerate(train_dataloader):\n        x = image.to(device)\n        y = label.to(device)\n        \n        model.eval()\n'''        \n        with torch.no_grad():\n            for j, [image,label] in enumerate(val_dataloader):\n                x = image.to(device)\n                y = label.to(device)\n\n                pred = model(x).argmax(dim=-1)\n\n                for i in range(len(pred)):\n                    y_true.append(y[i].item())\n                    y_pred.append(pred[i].item())\n'''\n#model.eval(x,y)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T13:24:16.286222Z","iopub.execute_input":"2022-08-31T13:24:16.286623Z","iopub.status.idle":"2022-08-31T13:29:40.489244Z","shell.execute_reply.started":"2022-08-31T13:24:16.286585Z","shell.execute_reply":"2022-08-31T13:29:40.487556Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"y_true = list()\ny_pred = list()\n\nwith torch.no_grad():\n    for j, [image,label] in enumerate(val_dataloader):\n        x = image.to(device)\n        y = label.to(device)\n        \n        pred = model(x).argmax(dim=-1)\n        \n        for i in range(len(pred)):\n            y_true.append(y[i].item())\n            y_pred.append(pred[i].item())\n            \nnp.mean(y_pred)\n\nnp.mean(y_true)","metadata":{},"execution_count":null,"outputs":[]}]}
